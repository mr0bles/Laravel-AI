### Variables
@defaultModel = deepseek-coder-v2:lite

### LLM Endpoints

### Generar respuesta con LLM
POST {{baseUrl}}/llm/generate
Content-Type: application/json

{
    "prompt": "¿Cuál es la capital de Francia?",
    "model": "{{defaultModel}}",
    "options": {
        "temperature": 0.7,
        "top_p": 0.9
    }
}

### Obtener lista de modelos disponibles
GET {{baseUrl}}/llm/models

### Obtener información de un modelo específico
GET {{baseUrl}}/llm/models/{{defaultModel}}
